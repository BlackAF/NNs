{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Conv2D, Layer, Conv2DTranspose, Flatten, Dense, Reshape, LeakyReLU, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Lambda, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 1024\n",
    "NOISE_DIM = 100\n",
    "\n",
    "HP = {\n",
    "    'batch_size': 256,\n",
    "    'buffer_size': 1024,\n",
    "    'noise_dim': 100,\n",
    "    'g_optimizer': Adam,\n",
    "    'd_optimizer': Adam,\n",
    "    'g_lr': 0.0002,\n",
    "    'd_lr': 0.0002,\n",
    "    'g_beta1': 0.5,\n",
    "    'd_beta1': 0.5,\n",
    "    'g_metrics': [BinaryAccuracy(name='g_accuracy', threshold=0.75)],\n",
    "    'd_metrics': [BinaryAccuracy(name='d_accuracy', threshold=0.75)],\n",
    "    'g_loss_fn': 'binary_crossentropy',\n",
    "    'd_loss_fn': 'binary_crossentropy',\n",
    "    'num_images_to_log': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Load data\n",
    "    (X_train, _), (X_test,_) = mnist.load_data()\n",
    "\n",
    "    # Preprocess data\n",
    "    X = tnp.vstack((X_train, X_test))\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    X = tf.reshape(X, (-1, 28, 28, 1))\n",
    "    y = tf.ones((X.shape[0], 1))\n",
    "\n",
    "    ds = Dataset.from_tensor_slices((X, y))\n",
    "    ds = ds.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 11s 100ms/step - g_loss: 1.7088 - d_loss: 0.2923 - loss: 0.4883 - g_accuracy: 0.0000e+00 - d_accuracy: 0.7409\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 10s 100ms/step - g_loss: 1.3730 - d_loss: 0.6955 - loss: 0.7870 - g_accuracy: 0.0958 - d_accuracy: 0.4655\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 10s 99ms/step - g_loss: 1.0655 - d_loss: 0.6114 - loss: 0.5429 - g_accuracy: 0.0459 - d_accuracy: 0.4223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb5ff6d6d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset()\n",
    "\n",
    "tm = TrainManager(log_dir='logs', experiment='test')\n",
    "\n",
    "gan, last_epoch = tm.init_gan(restore_latest=False)\n",
    "\n",
    "callbacks = [\n",
    "    LogGeneratedResults(num_images=HP['num_images_to_log'], log_dir=tm.generated_results_dir),\n",
    "    TensorBoard(log_dir=tm.tensorboard_dir, update_freq='epoch'),\n",
    "    SaveModel(log_dir=tm.model_dir)\n",
    "]\n",
    "\n",
    "gan.fit(ds.take(100), epochs=3, callbacks=callbacks, initial_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager:\n",
    "    def __init__(self, log_dir, experiment, create_dir=True):\n",
    "        self.log_dir = log_dir\n",
    "        self.experiment = experiment\n",
    "        \n",
    "        # Create the logs directory if it doesn't exist\n",
    "        if create_dir:\n",
    "            pathlib.Path(self.experiment_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def init_timestamp(self, latest=False):\n",
    "        if latest:\n",
    "            all_timestamps = os.listdir(self.experiment_dir)\n",
    "            if len(all_timestamps) == 0:\n",
    "                raise ValueError(f'Cannot restore latest as there are no timestamps in dir: \"{self.experiment_dir}\"')\n",
    "            timestamp = max(all_timestamps)\n",
    "        else:\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "            \n",
    "        self._curr_timestamp = timestamp\n",
    "        \n",
    "    def init_gan(self, restore_latest=False):\n",
    "        if not hasattr(self, '_curr_timestamp'):\n",
    "            self.init_timestamp(restore_latest)\n",
    "\n",
    "        if restore_latest:\n",
    "            gan = GAN.load(self.model_dir, latest=True)\n",
    "            return gan, self.last_epoch\n",
    "        else:\n",
    "            gan = build_gan()\n",
    "            gan.save_architecture(self.model_dir)\n",
    "            last_epoch = 0\n",
    "            return gan, last_epoch\n",
    "        \n",
    "    @property\n",
    "    def experiment_dir(self):\n",
    "        return os.path.join(self.log_dir, self.experiment)\n",
    "        \n",
    "    @property\n",
    "    def timestamp_dir(self):\n",
    "        return os.path.join(self.experiment_dir, self._curr_timestamp)\n",
    "        \n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return os.path.join(self.timestamp_dir, 'models')\n",
    "    \n",
    "    @property\n",
    "    def tensorboard_dir(self):\n",
    "        return os.path.join(self.timestamp_dir, 'tensorboard')\n",
    "    \n",
    "    @property\n",
    "    def generated_results_dir(self):\n",
    "        return os.path.join(self.timestamp_dir, 'generated_results')\n",
    "    \n",
    "    @property\n",
    "    def last_epoch(self):\n",
    "        # Both the discrimator and generator have the same # of epochs so we can get either\n",
    "        latest_ckpt_path = tf.train.latest_checkpoint(os.path.join(self.model_dir, 'discriminator'))\n",
    "        path = pathlib.PurePath(latest_ckpt_path)\n",
    "        # Checkpoint prefixes are stored as epoch_x so we split to get the epoch number\n",
    "        epoch = path.name.split('_')[1]\n",
    "        return int(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan():\n",
    "    generator = build_generator(HP['noise_dim'])\n",
    "    discriminator = build_discriminator(input_shape=(28, 28, 1))\n",
    "\n",
    "    gan = GAN(generator, discriminator, HP['noise_dim'])\n",
    "    gan.compile(\n",
    "        g_optimizer=HP['g_optimizer'](lr=HP['g_lr'], beta_1=HP['g_beta1']), \n",
    "        d_optimizer=HP['d_optimizer'](lr=HP['d_lr'], beta_1=HP['d_beta1']),\n",
    "        g_metrics=HP['g_metrics'],\n",
    "        d_metrics=HP['d_metrics'],\n",
    "        loss_fn=HP['g_loss_fn']\n",
    "    )\n",
    "\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(noise_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    num_features = 128\n",
    "    \n",
    "    # 7x7\n",
    "    model.add(Dense(num_features * 7 * 7, input_dim=noise_dim))\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "    model.add(Reshape((7, 7, num_features)))\n",
    "    \n",
    "    # 14x14\n",
    "    model.add(Conv2DTranspose(num_features, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "    \n",
    "    # 28x28\n",
    "    model.add(Conv2DTranspose(num_features, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "    \n",
    "    model.add(Conv2D(1, (7, 7), activation='sigmoid', padding='same'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "#     model.add(MinibatchDiscrimination(1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(Model):\n",
    "    def __init__(self, generator, discriminator, noise_dim, **kwargs):\n",
    "        super(GAN, self).__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.noise_dim = noise_dim\n",
    "        \n",
    "    def compile(self, g_optimizer, d_optimizer, g_metrics, d_metrics, loss_fn, **kwargs):\n",
    "        super(GAN, self).compile(**kwargs)\n",
    "        \n",
    "        self.generator.compile(optimizer=g_optimizer, metrics=g_metrics, loss=loss_fn)\n",
    "        self.discriminator.compile(optimizer=d_optimizer, metrics=d_metrics, loss=loss_fn)\n",
    "        \n",
    "        # Save the compiled info so we can deserialize the model later\n",
    "        self.compiled_config = {\n",
    "            'g_optimizer': self.generator.optimizer,\n",
    "            'd_optimizer': self.discriminator.optimizer,\n",
    "            'g_metrics': g_metrics,\n",
    "            'd_metrics': d_metrics,\n",
    "            'loss_fn': loss_fn,\n",
    "            **kwargs\n",
    "        }\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        X_real, y_real = data\n",
    "        batch_size = tf.shape(X_real)[0]\n",
    "\n",
    "        X_fake, y_fake = self.generate_fake_data(batch_size, self.noise_dim)\n",
    "        \n",
    "        # Should shuffle? so that it's not all real then all fake\n",
    "        X, y = tnp.vstack((X_real, X_fake)), tnp.vstack((y_real, y_fake))\n",
    "    \n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.discriminator(X)\n",
    "            d_loss = self.discriminator.compiled_loss(y, y_pred, regularization_losses=self.discriminator.losses) \n",
    "        d_gradients = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.discriminator.optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_weights))\n",
    "    \n",
    "        # Update discriminator metrics\n",
    "        self.discriminator.compiled_metrics.update_state(y_real, self.discriminator(X_real))        \n",
    "        \n",
    "        y_gan = tf.ones((batch_size, 1))\n",
    "        \n",
    "        # Train generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            X_gan, _ = self.generate_fake_data(batch_size, self.noise_dim)\n",
    "            y_pred = self.discriminator(X_gan)\n",
    "            g_loss = self.generator.compiled_loss(y_gan, y_pred, regularization_losses=self.generator.losses)\n",
    "        g_gradients = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.generator.optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_weights))\n",
    "        \n",
    "        # Update generator metrics\n",
    "        self.generator.compiled_metrics.update_state(y_gan, y_pred)\n",
    "\n",
    "        return {\n",
    "            'g_loss': g_loss,\n",
    "            'd_loss': d_loss,\n",
    "            **{m.name: m.result() for m in self.generator.metrics},\n",
    "            **{m.name: m.result() for m in self.discriminator.metrics}\n",
    "        }\n",
    "\n",
    "    def generate_fake_data(self, batch_size, noise_dim):\n",
    "        noise_data = tf.random.normal(shape=(batch_size, noise_dim))\n",
    "        X_fake = self.generator(noise_data)\n",
    "        y_fake = tf.zeros((batch_size, 1))\n",
    "        \n",
    "        return X_fake, y_fake\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # Resets these metrics after each epoch        \n",
    "        return [*self.generator.metrics, *self.discriminator.metrics]\n",
    "    \n",
    "    def save_architecture(self, log_dir):\n",
    "        pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(log_dir, 'gan_architecture.json'), 'w') as json_file:\n",
    "            json_file.write(self.to_json())\n",
    "    \n",
    "    def save_weights(self, log_dir, prefix):\n",
    "        self.generator.save_weights(os.path.join(log_dir, 'generator', prefix))\n",
    "        self.discriminator.save_weights(os.path.join(log_dir, 'discriminator', prefix))\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, log_dir, prefix=None, latest=False):\n",
    "        if not latest and not prefix:\n",
    "            raise ValueError('Either prefix or latest should be used.')\n",
    "            \n",
    "        with tf.keras.utils.custom_object_scope({'GAN': cls}):\n",
    "            saved_json = open(os.path.join(log_dir, 'gan_architecture.json'), 'r').read()\n",
    "            gan = model_from_json(saved_json)\n",
    "            \n",
    "        if latest:\n",
    "            generator_ckpt = tf.train.latest_checkpoint(os.path.join(log_dir, 'generator'))\n",
    "            discriminator_ckpt = tf.train.latest_checkpoint(os.path.join(log_dir, 'discriminator'))\n",
    "        else:\n",
    "            generator_ckpt = os.path.join(log_dir, 'generator', prefix)\n",
    "            discriminator_ckpt = os.path.join(log_dir, 'discriminator', prefix)\n",
    "\n",
    "        gan.generator.load_weights(generator_ckpt)\n",
    "        gan.discriminator.load_weights(discriminator_ckpt)\n",
    "        \n",
    "        return gan\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'generator': self.generator,\n",
    "            'discriminator': self.discriminator,\n",
    "            'noise_dim': self.noise_dim,\n",
    "            'compiled_config': self.compiled_config\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        generator = model_from_json(json.dumps(config.pop('generator')))\n",
    "        discriminator = model_from_json(json.dumps(config.pop('discriminator')))\n",
    "        \n",
    "        compiled_config = config.pop('compiled_config')\n",
    "        compiled_config['g_optimizer'] = tf.keras.optimizers.deserialize(compiled_config['g_optimizer'])\n",
    "        compiled_config['d_optimizer'] = tf.keras.optimizers.deserialize(compiled_config['d_optimizer'])\n",
    "        compiled_config['g_metrics'] = [tf.keras.metrics.deserialize(c) for c in compiled_config['g_metrics']]\n",
    "        compiled_config['d_metrics'] = [tf.keras.metrics.deserialize(c) for c in compiled_config['d_metrics']]\n",
    "        compiled_config['loss_fn'] = tf.keras.losses.deserialize(compiled_config['loss_fn'])\n",
    "\n",
    "        gan = cls(generator, discriminator, **config)\n",
    "        gan.compile(**compiled_config)\n",
    "        \n",
    "        return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Generated Results Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogGeneratedResults(Callback):\n",
    "    def __init__(self, num_images, log_dir, **kwargs):\n",
    "        super(LogGeneratedResults, self).__init__(**kwargs)\n",
    "        self.num_images = num_images\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if not hasattr(self.model, 'generate_fake_data'):\n",
    "            raise ValueError('The model should have a generate_fake_data function.')\n",
    "            \n",
    "        if not hasattr(self.model, 'noise_dim'):\n",
    "            raise ValueError('The model should have a noise_dim property.')\n",
    "\n",
    "        images, _ = self.model.generate_fake_data(self.num_images, self.model.noise_dim)\n",
    "        title = f'Total Images: {self.num_images} | Noise Dim: {self.model.noise_dim}'\n",
    "\n",
    "        self.log_images(images, title, self.log_dir, 'gray', epoch)\n",
    "\n",
    "    def log_images(self, images, title, log_dir, cmap, epoch):\n",
    "        assert len(images.shape) == 4\n",
    "\n",
    "        num_images = len(images)\n",
    "        figsize = int(np.ceil(np.sqrt(num_images)))\n",
    "\n",
    "        figure = plt.figure(figsize=(figsize, figsize))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(figsize, figsize, i + 1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(images[i, :, :, :], cmap=cmap)\n",
    "\n",
    "        # Save the plot to a PNG in memory.\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        # Closing the figure prevents it from being displayed directly inside\n",
    "        # the notebook.\n",
    "        plt.close(figure)\n",
    "        buf.seek(0)\n",
    "        # Convert PNG buffer to TF image\n",
    "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "        # Add the batch dimension\n",
    "        image = tf.expand_dims(image, 0)\n",
    "\n",
    "        file_writer = tf.summary.create_file_writer(log_dir, name='test')\n",
    "\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.image(title, image, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModel(Callback):\n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super(SaveModel, self).__init__(**kwargs)\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.model.save_weights(log_dir=self.log_dir, prefix=f'epoch_{epoch+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Discrimination Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchDiscrimination(Layer):\n",
    "    def __init__(self, kernel_dims, **kwargs):\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "        self.kernel_dims = kernel_dims\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_features = input_shape[-1]\n",
    "    \n",
    "    def call(self, X):\n",
    "        features = tf.reshape(X, (-1, self.in_features, self.kernel_dims)) # NxBxC\n",
    "\n",
    "        Mi = tf.expand_dims(features, axis=0) # 1xNxBxC\n",
    "\n",
    "        Mj = tf.expand_dims(features, axis=1) # Nx1xBxC\n",
    "\n",
    "        abs_diff = tnp.abs(Mi - Mj) # NxNxBxC\n",
    "\n",
    "        norm = tnp.sum(abs_diff, axis=3) # NxNxB\n",
    "    \n",
    "        outputs = tnp.sum(tnp.exp(-norm), axis=0) # NxB\n",
    "\n",
    "        return Concatenate(axis=1)((X, outputs)) # Nx(B+X.shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(MinibatchDiscrimination, self).get_config()\n",
    "        config.update({ 'kernel_dims': self.kernel_dims })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
