{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import pathlib\n",
    "import io\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg19_preprocess_input\n",
    "from tensorflow.keras.models import Sequential, Model, clone_model\n",
    "from tensorflow.keras.layers import Dense, Input, Layer, Flatten, Lambda, UpSampling2D, Conv2D, Dropout\n",
    "from tensorflow.keras.losses import Loss, MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.constraints import MinMaxNorm\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.io.gfile import GFile\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "# Layers to take from VGG19\n",
    "STYLE_LAYERS = ('block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2',\n",
    "                'block3_conv3', 'block3_conv4', 'block3_pool')\n",
    "\n",
    "DECODER_LAYERS = ('block3_pool', 'block3_conv1', 'block2_pool', 'block2_conv1', 'block1_pool', 'block1_conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    def parse_img(file_name):\n",
    "        img = tf.io.read_file(file_name)\n",
    "        img = tf.io.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [224, 224], antialias=True, method='nearest')\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = vgg19_preprocess_input(img) / 255.0\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def build_ds(file_names):\n",
    "        tmp_ds = Dataset.from_tensor_slices(file_names)\n",
    "        tmp_ds = tmp_ds.shuffle(len(file_names))\n",
    "        tmp_ds = tmp_ds.map(parse_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return tmp_ds\n",
    "    \n",
    "    content_ds = build_ds(glob.glob('/home/jephthia/datasets/mscoco/unlabeled2017/train/*')[:1])\n",
    "    # style_ds = build_ds(glob.glob('/home/jephthia/datasets/wikiart/train/*')[:1])\n",
    "    \n",
    "    ds = content_ds.batch(BATCH_SIZE)\n",
    "   \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = VGG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModel(Callback):\n",
    "    def __init__(self, log_dir, update_freq=1, **kwargs):\n",
    "        super(SaveModel, self).__init__(**kwargs)\n",
    "        self.log_dir = log_dir\n",
    "        self.update_freq = update_freq\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        epoch = epoch + 1\n",
    "        if epoch % self.update_freq == 0:\n",
    "            self.model.save_model(log_dir=os.path.join(self.log_dir, f'epoch_{epoch}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DReflectivePadding(Layer):\n",
    "    def __init__(self, *args, conv2d=None, **kwargs):\n",
    "        super().__init__(name=kwargs.get('name', None))\n",
    "        \n",
    "        if conv2d is None:\n",
    "            self.conv2d = Conv2D(*args, **kwargs)\n",
    "        else:\n",
    "            self.conv2d = conv2d\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.pad(x, tf.constant([[0, 0], [1, 1], [1, 1], [0, 0]]), 'REFLECT')\n",
    "        x = self.conv2d(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'conv2d': self.conv2d})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(conv2d=config['conv2d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransfer(Model):\n",
    "    def __init__(self, *args, encoder=None, decoder=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.init_encoder(encoder)\n",
    "        self.init_decoder(decoder)\n",
    "        self.build_metrics()\n",
    "    \n",
    "    def init_encoder(self, encoder):\n",
    "        if encoder is not None:\n",
    "            self.encoder = encoder\n",
    "            return\n",
    "\n",
    "        self.encoder = Sequential([Input((224,224,3))])\n",
    "        self.encoder.trainable = False\n",
    "\n",
    "        for layer_name in STYLE_LAYERS:\n",
    "            layer = vgg19.get_layer(layer_name)\n",
    "            self.encoder.add(layer)\n",
    "            \n",
    "        self.encoder.compile()\n",
    "        \n",
    "    def init_decoder(self, decoder):\n",
    "        if decoder is not None:\n",
    "            self.decoder = decoder\n",
    "            return\n",
    "\n",
    "        # Build the decoder if it wasn't provided\n",
    "        input_shape = self.encoder.layers[-1].output_shape[1:]\n",
    "        self.decoder = Sequential([Input(input_shape)])\n",
    "\n",
    "        # The decoder is the trimed inverse of the encoder\n",
    "        for layer_name in DECODER_LAYERS:\n",
    "            layer = vgg19.get_layer(layer_name)\n",
    "            # Add the upsampling to double the image size                \n",
    "            if 'pool' in layer.name:\n",
    "                block_name = layer.name.split(\"_\")[0]\n",
    "                self.decoder.add(UpSampling2D(name=f'{block_name}_upsampling'))\n",
    "            # Add some reflective padding followed by a Conv2D layer\n",
    "            elif 'conv' in layer.name:\n",
    "                self.decoder.add(Conv2DReflectivePadding(\n",
    "                    filters=layer.output_shape[-1],\n",
    "                    kernel_size=layer.kernel_size,\n",
    "                    strides=layer.strides,\n",
    "                    activation='relu',\n",
    "                    name=layer.name))\n",
    "                \n",
    "        # Add one final Conv2D to reduce the feature maps to 3 (N,W,H,3)\n",
    "        self.decoder.add(Conv2DReflectivePadding(3, (3,3), name='output_conv1'))\n",
    "    \n",
    "    def build_metrics(self):\n",
    "        self.c_loss_metric = Mean(name='c_loss')\n",
    "\n",
    "    def compile(self, optimizer, content_loss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        if not getattr(self, 'decoder_compiled', False):\n",
    "             self.decoder.compile(optimizer=optimizer)\n",
    "        self.content_loss = content_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data, training=True):\n",
    "        c_encoded_outputs = data\n",
    "            \n",
    "        with tf.GradientTape(watch_accessed_variables=training) as tape:\n",
    "            # 1. Encode the content and style image\n",
    "            for layer in self.encoder.layers:\n",
    "                # Encode the content image\n",
    "                c_encoded_outputs = layer(c_encoded_outputs)\n",
    "\n",
    "            # 3. Decode the feature maps generated by AdaIN to get the final generated image\n",
    "            generated_imgs = self.decoder(c_encoded_outputs, training=training)\n",
    "\n",
    "            # 4. Encode the generated image to calculate the loss\n",
    "            g_encoded_outputs = generated_imgs\n",
    "            for layer in self.encoder.layers:\n",
    "                # Encode the generated image\n",
    "                g_encoded_outputs = layer(g_encoded_outputs)\n",
    "                    \n",
    "            # 5. Calculate the content loss\n",
    "            c_per_replica_loss = self.content_loss(g_encoded_outputs, c_encoded_outputs) # (N,W,H)\n",
    "            # Reduce the loss (we do this ourselves in order to be compatible with distributed training)\n",
    "            global_c_loss_size = tf.size(c_per_replica_loss) * self.distribute_strategy.num_replicas_in_sync\n",
    "            global_c_loss_size = tf.cast(global_c_loss_size, dtype=tf.float32)\n",
    "            c_loss = tf.nn.compute_average_loss(c_per_replica_loss, global_batch_size=global_c_loss_size)\n",
    "\n",
    "        # 8. Apply gradient descent\n",
    "        if training:\n",
    "            gradients = tape.gradient(c_loss, self.decoder.trainable_variables)\n",
    "#             gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "#             tf.print('---')\n",
    "#             tf.print('glonorm', tf.linalg.global_norm(gradients))\n",
    "#             tf.print(list((i, tf.math.reduce_min(n), tf.math.reduce_max(n)) for i,n in enumerate(gradients)))\n",
    "#             tf.print(list((i, tf.math.reduce_min(n), tf.math.reduce_max(n)) for i,n in enumerate(self.decoder.trainable_variables)))\n",
    "#             tf.print('C_ENC --> ', tf.math.reduce_min(c_encoded_outputs), tf.math.reduce_max(c_encoded_outputs))\n",
    "#             tf.print('GEN_IMG --> ', tf.math.reduce_min(generated_imgs), tf.math.reduce_max(generated_imgs))\n",
    "#             tf.print('G_ENC --> ', tf.math.reduce_min(g_encoded_outputs), tf.math.reduce_max(g_encoded_outputs))\n",
    "\n",
    "            self.decoder.optimizer.apply_gradients(zip(gradients, self.decoder.trainable_variables))\n",
    "        \n",
    "        # 9. Update the metrics\n",
    "        self.c_loss_metric.update_state(c_loss)\n",
    "\n",
    "        return { m.name: m.result() for m in self.metrics }\n",
    "\n",
    "    @tf.function\n",
    "    def predict_step(self, data):\n",
    "        content_imgs = data\n",
    "        \n",
    "        # Ensure these are batched\n",
    "        assert len(content_imgs.shape) == 4\n",
    "\n",
    "        content_imgs = vgg19_preprocess_input(content_imgs) / 255.0\n",
    "\n",
    "        c_encoded = content_imgs\n",
    "\n",
    "        # Encode the contents and styles\n",
    "        for layer in self.encoder.layers:\n",
    "            c_encoded = layer(c_encoded)\n",
    "        \n",
    "        # Decode the images to generate them\n",
    "        generated_imgs = self.decoder(c_encoded)\n",
    "        generated_imgs = self.deprocess_vgg19(generated_imgs)\n",
    "        \n",
    "        return generated_imgs\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.c_loss_metric]\n",
    "    \n",
    "    def deprocess_vgg19(self, imgs):\n",
    "        # Ensure they are batched\n",
    "        assert len(imgs.shape) == 4\n",
    "        \n",
    "        # Put back to 0...255\n",
    "        imgs *= 255.0\n",
    "        # Add mean\n",
    "        imgs += [103.939, 116.779, 123.68]\n",
    "        # BGR to RGB\n",
    "        imgs = imgs[..., ::-1]\n",
    "        # Clip\n",
    "        imgs = tf.clip_by_value(imgs, 0.0, 255.0)\n",
    "        # Cast\n",
    "        imgs = tf.cast(imgs, tf.uint8)\n",
    "\n",
    "        return imgs\n",
    "    \n",
    "    def save_architecture(self, log_dir):\n",
    "        # Ensure the log_dir exists\n",
    "        pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with GFile(os.path.join(log_dir, 'style_transfer_architecture.json'), 'w') as f:\n",
    "            f.write(self.to_json())\n",
    "            \n",
    "    def save_encoder(self, log_dir):\n",
    "        self.encoder.save(log_dir)\n",
    "    \n",
    "    def save_model(self, log_dir, **kwargs):\n",
    "        self.decoder.save(log_dir, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, log_dir=None, epoch=None):\n",
    "        model_found = bool(log_dir) and pathlib.Path(os.path.join(log_dir, 'style_transfer_architecture.json')).is_file()\n",
    "                \n",
    "        # If there isn't already a model create one from scratch and save it\n",
    "        if not model_found:\n",
    "            model = cls()\n",
    "            if log_dir:\n",
    "                model.save_architecture(log_dir)\n",
    "                model.save_encoder(os.path.join(log_dir, 'encoder'))\n",
    "            return model\n",
    "        \n",
    "        # Load the model's architecture\n",
    "        with tf.keras.utils.custom_object_scope({'StyleTransfer':cls, 'Conv2DReflectivePadding':Conv2DReflectivePadding}):\n",
    "            saved_json = GFile(os.path.join(log_dir, 'style_transfer_architecture.json'), 'r').read()\n",
    "            model = tf.keras.models.model_from_json(saved_json)\n",
    "            model.encoder = tf.keras.models.load_model(os.path.join(log_dir, 'encoder'))\n",
    "        \n",
    "        # If an epoch was provided, load the model at that epoch\n",
    "        if epoch is not None:\n",
    "            epoch_path = os.path.join(log_dir, 'weights', f'epoch_{epoch}')\n",
    "            if not pathlib.Path(epoch_path).is_dir():\n",
    "                print(f\"Epoch {epoch} doesn't exists\")\n",
    "                return\n",
    "            print('Loading Checkpoint:', epoch_path)\n",
    "            model.decoder = tf.keras.models.load_model(epoch_path)\n",
    "            model.decoder_compiled = True\n",
    "        else:\n",
    "            # Load the decoder's latest weights if there are any\n",
    "            ckpts = glob.glob(os.path.join(log_dir, 'weights', '*'))\n",
    "            if ckpts:\n",
    "                latest_ckpt = max(ckpts, key=os.path.getmtime)\n",
    "                print('Loading Checkpoint:', latest_ckpt)\n",
    "                model.decoder = tf.keras.models.load_model(latest_ckpt)\n",
    "                model.decoder_compiled = True\n",
    "\n",
    "        return model\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'encoder': self.encoder,\n",
    "            'decoder': self.decoder\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config, **kwargs):\n",
    "        encoder = tf.keras.models.model_from_json(json.dumps(config.pop('encoder')))\n",
    "        decoder = tf.keras.models.model_from_json(json.dumps(config.pop('decoder')))\n",
    "\n",
    "        style_transfer = cls(encoder=encoder, decoder=decoder)\n",
    "        return style_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_epoch(log_dir=None):\n",
    "    if log_dir is None:\n",
    "        return 0\n",
    "\n",
    "    ckpts = glob.glob(os.path.join(log_dir, 'weights', '*'))\n",
    "\n",
    "    # If there are no latest checkpoints start from scratch\n",
    "    if not ckpts:\n",
    "        return 0\n",
    "    \n",
    "    latest_ckpt_path = max(ckpts, key=os.path.getmtime)\n",
    "    path = pathlib.PurePath(latest_ckpt_path)\n",
    "    # Checkpoint prefixes are stored as epoch_x so we split to get the epoch number\n",
    "    epoch = path.name.split('_')[-1]\n",
    "    \n",
    "    return int(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = None\n",
    "# log_dir = 'testlogs/simple_adain/adam_255'\n",
    "\n",
    "st = StyleTransfer.load(log_dir)\n",
    "st.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-02),\n",
    "    content_loss=MeanSquaredError(reduction='none'),\n",
    ")\n",
    "\n",
    "# st.decoder.optimizer.lr = 1e-05\n",
    "\n",
    "ds = get_dataset()\n",
    "\n",
    "callbacks = [\n",
    "#     EarlyStopping(monitor='loss', patience=3),\n",
    "#     ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, cooldown=2),\n",
    "#     TensorBoard(log_dir=f'{log_dir}/tensorboard', update_freq=100),\n",
    "#     SaveModel(log_dir=f'{log_dir}/weights', update_freq=100),\n",
    "#     SaveWeightsSummary(f'{log_dir}/tensorboard/weights')`,\n",
    "#     LRDecay(1e-3, 5e-04, 1, 1000),\n",
    "#     LRSearch(1e-3, 1e-07, 1, 300),\n",
    "]\n",
    "\n",
    "last_epoch = get_last_epoch(log_dir)\n",
    "\n",
    "print('LR', st.decoder.optimizer.lr.numpy())\n",
    "\n",
    "epochs = 300\n",
    "st.fit(ds, epochs=epochs+last_epoch, initial_epoch=last_epoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.decoder.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.decoder.optimizer.lr = 1e-05\n",
    "last_epoch=40\n",
    "epochs=50\n",
    "st.fit(ds, epochs=epochs+last_epoch, initial_epoch=last_epoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRDecay(Callback):\n",
    "    def __init__(self, lr_min, lr_max, steps, epochs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.steps = steps\n",
    "        self.epochs = epochs\n",
    "        self.decay = (lr_max / lr_min) ** (1/(epochs*steps))\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.model.decoder.optimizer.lr = self.lr = self.lr_min\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs):\n",
    "        self.lr *= self.decay\n",
    "        self.model.decoder.optimizer.lr = self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSearch(Callback):\n",
    "    def __init__(self, lr_min, lr_max, steps, epochs, log_dir=None, save=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.steps = steps\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.save = save\n",
    "\n",
    "        self.decay = (lr_max / lr_min) ** (1/(epochs*steps))\n",
    "        self.history = {'lr': [], 'loss': []}\n",
    "        self.best_loss = math.inf\n",
    "        self.best_lr = math.inf\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.model.decoder.optimizer.lr = self.lr = self.lr_min\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs):\n",
    "        self.lr *= self.decay\n",
    "        self.model.decoder.optimizer.lr = self.lr\n",
    "        \n",
    "        self.history['lr'].append(self.lr)\n",
    "        self.history['loss'].append(logs['c_loss'])\n",
    "        \n",
    "        if self.save:\n",
    "            self.model.save_weights(os.path.join(self.log_dir, 'tmp_lr_search', str(len(self.history['loss']) - 1)))\n",
    "        \n",
    "        if self.best_loss > logs['c_loss']:\n",
    "            self.best_loss = logs['c_loss']\n",
    "            self.best_loss_index = len(self.history['loss']) - 1\n",
    "            self.best_lr = self.history['lr'][self.best_loss_index - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_fn = glob.glob('/home/jephthia/datasets/mscoco/unlabeled2017/train/*')[0]\n",
    "\n",
    "def parseim(file_name):\n",
    "    img = tf.io.read_file(file_name)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224], antialias=True, method='nearest')\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "    return img\n",
    "\n",
    "c_imgs = parseim(content_fn)\n",
    "\n",
    "loadedst = StyleTransfer.load('testlogs/simple_adain/adam', epoch=3500)\n",
    "pred = loadedst.predict(c_imgs, batch_size=1)\n",
    "\n",
    "plt.imshow(tf.cast(c_imgs[0], tf.uint8))\n",
    "plt.show()\n",
    "plt.imshow(pred[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min = 1e-03\n",
    "lr_max = 1e-07\n",
    "epochs = 100\n",
    "steps = 1\n",
    "decay = (lr_max / lr_min) ** (1/(epochs*steps))\n",
    "\n",
    "lr = lr_min\n",
    "\n",
    "his = []\n",
    "for i in range(epochs):\n",
    "    lr *= decay\n",
    "    his.append(lr)\n",
    "    \n",
    "plt.plot(his)\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
