{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import pathlib\n",
    "import io\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "from tensorflow_io import IODataset\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg19_preprocess_input\n",
    "from tensorflow.keras.models import Sequential, Model, clone_model\n",
    "from tensorflow.keras.layers import Dense, Input, Layer, Flatten, Lambda, UpSampling2D, Conv2D, Dropout\n",
    "from tensorflow.keras.losses import Loss, MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.constraints import MinMaxNorm\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.io.gfile import GFile\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "STYLE_WEIGHT = 0.8\n",
    "# Layers to take from VGG19\n",
    "STYLE_LAYERS = ('block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2',\n",
    "                'block3_conv3', 'block3_conv4', 'block3_pool')\n",
    "DECODER_LAYERS = ('block3_pool', 'block3_conv1', 'block2_pool', 'block2_conv1', 'block1_pool', 'block1_conv1')\n",
    "# STYLE_LAYERS = ('block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2',\n",
    "#                 'block3_conv3', 'block3_conv4', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_conv4', 'block4_pool')\n",
    "\n",
    "# Layers used to calculate the style loss\n",
    "# LOSS_LAYERS = ('block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1')\n",
    "LOSS_LAYERS = (['block1_conv1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    def parse_img(file_name):\n",
    "        img = tf.io.read_file(file_name)\n",
    "        img = tf.io.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [224, 224], antialias=True, method='nearest')\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = vgg19_preprocess_input(img) / 255.0\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def build_ds(file_names):\n",
    "        tmp_ds = Dataset.from_tensor_slices(file_names)\n",
    "        tmp_ds = tmp_ds.shuffle(len(file_names))\n",
    "        tmp_ds = tmp_ds.map(parse_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return tmp_ds\n",
    "    \n",
    "    content_ds = build_ds(glob.glob('/home/jephthia/datasets/mscoco/unlabeled2017/train/*')[:1])\n",
    "    style_ds = build_ds(glob.glob('/home/jephthia/datasets/wikiart/train/*')[:1])\n",
    "    val_content_ds = build_ds(glob.glob('/home/jephthia/datasets/mscoco/unlabeled2017/validate/*')[:2500])\n",
    "    val_style_ds = build_ds(glob.glob('/home/jephthia/datasets/wikiart/validate/*')[:2500])\n",
    "    \n",
    "    # Train dataset\n",
    "    ds = Dataset.zip((content_ds, style_ds))\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "#     ds = ds.prefetch(2)\n",
    "    \n",
    "    # Validation dataset\n",
    "    val_ds = Dataset.zip((val_content_ds, val_style_ds))\n",
    "    val_ds = val_ds.batch(BATCH_SIZE)\n",
    "#     val_ds = val_ds.cache()\n",
    "#     val_ds = val_ds.prefetch(2)\n",
    "   \n",
    "    return ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = VGG19()\n",
    "\n",
    "# vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModel(Callback):\n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super(SaveModel, self).__init__(**kwargs)\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.model.save_model(log_dir=os.path.join(self.log_dir, f'epoch_{epoch+1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveWeightsSummary(Callback):\n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_dir = log_dir\n",
    "        # Create directory if it doesn't exists\n",
    "        pathlib.Path(self.log_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        for layer in self.model.decoder.layers:\n",
    "            if layer.weights:\n",
    "                with h5py.File(os.path.join(self.log_dir, 'weights.hdf5'), 'a') as f:\n",
    "                    group = f[layer.name] if layer.name in f else f.create_group(layer.name, track_order=True)\n",
    "                    kernel_group = group['kernel'] if 'kernel' in group else group.create_group('kernel')\n",
    "                    bias_group = group['bias'] if 'bias' in group else group.create_group('bias')\n",
    "\n",
    "                    kernel_group.create_dataset(str(batch), data=layer.weights[0].numpy(), dtype=np.float32)\n",
    "                    bias_group.create_dataset(str(batch), data=layer.weights[1].numpy(), dtype=np.float32)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DReflectivePadding(Layer):\n",
    "    def __init__(self, *args, conv2d=None, **kwargs):\n",
    "        super().__init__(name=kwargs.get('name', None))\n",
    "        \n",
    "        if conv2d is None:\n",
    "            self.conv2d = Conv2D(*args, **kwargs)\n",
    "        else:\n",
    "            self.conv2d = conv2d\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.pad(x, tf.constant([[0, 0], [1, 1], [1, 1], [0, 0]]), 'REFLECT')\n",
    "        x = self.conv2d(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'conv2d': self.conv2d})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(conv2d=config['conv2d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(Loss):\n",
    "    def __init__(self, *args, epsilon=1e-07, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mse = MeanSquaredError(reduction=self.reduction)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def call(self, g_encoded_outputs, s_encoded_outputs):\n",
    "        mean_g, variance_g = tf.nn.moments(g_encoded_outputs, axes=[1, 2])\n",
    "        mean_s, variance_s = tf.nn.moments(s_encoded_outputs, axes=[1, 2])\n",
    "\n",
    "        std_g = tf.math.sqrt(variance_g + self.epsilon)\n",
    "        std_s = tf.math.sqrt(variance_s + self.epsilon)\n",
    "\n",
    "        loss = self.mse(mean_g, mean_s) + self.mse(std_g, std_s)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, content, style, epsilon=1e-07):\n",
    "        # Calculates the mean and variance of the content image\n",
    "        c_mean, c_variance = tf.nn.moments(content, axes=[1, 2], keepdims=True)\n",
    "        # Calculates the standard deviation of the content image\n",
    "        c_std = tf.math.sqrt(c_variance + epsilon)\n",
    "        \n",
    "        # Calculates the mean and variance of the style image\n",
    "        s_mean, s_variance = tf.nn.moments(style, axes=[1, 2], keepdims=True)\n",
    "        # Calculates the standard deviation of the style image\n",
    "        s_std = tf.math.sqrt(s_variance + epsilon)\n",
    "\n",
    "        norm = (s_std * ((content - c_mean) / c_std)) + s_mean\n",
    "\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransfer(Model):\n",
    "    def __init__(self, *args, encoder=None, decoder=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.init_encoder(encoder)\n",
    "        self.init_decoder(decoder)\n",
    "        self.build_metrics()\n",
    "    \n",
    "    def init_encoder(self, encoder):\n",
    "        if encoder is not None:\n",
    "            self.encoder = encoder\n",
    "            return\n",
    "\n",
    "        self.encoder = Sequential([Input((224,224,3))])\n",
    "        self.encoder.trainable = False\n",
    "\n",
    "        for layer_name in STYLE_LAYERS:\n",
    "            layer = vgg19.get_layer(layer_name)\n",
    "            self.encoder.add(layer)\n",
    "            \n",
    "        self.encoder.compile()\n",
    "        \n",
    "    def init_decoder(self, decoder):\n",
    "        if decoder is not None:\n",
    "            self.decoder = decoder\n",
    "            return\n",
    "\n",
    "        # Build the decoder if it wasn't provided\n",
    "        input_shape = self.encoder.layers[-1].output_shape[1:]\n",
    "        self.decoder = Sequential([Input(input_shape)])\n",
    "\n",
    "        # The decoder is the trimed inverse of the encoder\n",
    "        for layer_name in DECODER_LAYERS:\n",
    "            layer = vgg19.get_layer(layer_name)\n",
    "            # Add the upsampling to double the image size                \n",
    "            if 'pool' in layer.name:\n",
    "                block_name = layer.name.split(\"_\")[0]\n",
    "                self.decoder.add(UpSampling2D(name=f'{block_name}_upsampling'))\n",
    "            # Add some reflective padding followed by a Conv2D layer\n",
    "            elif 'conv' in layer.name:\n",
    "                self.decoder.add(Conv2DReflectivePadding(\n",
    "                    filters=layer.output_shape[-1],\n",
    "                    kernel_size=layer.kernel_size,\n",
    "                    strides=layer.strides,\n",
    "                    activation='relu',\n",
    "                    name=layer.name))\n",
    "                \n",
    "        # Add one final Conv2D to reduce the feature maps to 3 (N,W,H,3)\n",
    "        self.decoder.add(Conv2DReflectivePadding(3, (3,3), name='output_conv1'))\n",
    "    \n",
    "    def build_metrics(self):\n",
    "        self.c_loss_metric = Mean(name='c_loss')\n",
    "        self.s_loss_metric = Mean(name='s_loss')\n",
    "        self.loss_metric = Mean(name='loss')\n",
    "\n",
    "    def compile(self, optimizer, content_loss, style_loss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        if not getattr(self, 'decoder_compiled', False):\n",
    "             self.decoder.compile(optimizer=optimizer)\n",
    "        self.content_loss = content_loss\n",
    "        self.style_loss = style_loss\n",
    "        self.adain = AdaIN()\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data, training=True):\n",
    "        c_encoded_outputs, s_encoded_outputs = data\n",
    "\n",
    "        # The outputs of the encoded style images and the encoded generated images\n",
    "        # Retrieved from the selected encoder layers used for the loss\n",
    "        s_loss_outputs = []\n",
    "        g_loss_outputs = []\n",
    "            \n",
    "        with tf.GradientTape(watch_accessed_variables=training) as tape:\n",
    "            # 1. Encode the content and style image\n",
    "            for layer in self.encoder.layers:\n",
    "                # Encode the content image\n",
    "                c_encoded_outputs = layer(c_encoded_outputs)\n",
    "                # Encode the style image\n",
    "                s_encoded_outputs = layer(s_encoded_outputs)\n",
    "\n",
    "                # If this layer is used to calculate the loss save its outputs\n",
    "                if layer.name in LOSS_LAYERS:\n",
    "                    s_loss_outputs.append(s_encoded_outputs)\n",
    "\n",
    "            # 2. Adaptive Instance Normalization\n",
    "            adain_outputs = self.adain(c_encoded_outputs, s_encoded_outputs)\n",
    "\n",
    "            # 3. Decode the feature maps generated by AdaIN to get the final generated image\n",
    "            generated_imgs = self.decoder(adain_outputs, training=training)\n",
    "\n",
    "            # 4. Encode the generated image to calculate the loss\n",
    "            g_encoded_outputs = generated_imgs\n",
    "            for layer in self.encoder.layers:\n",
    "                # Encode the generated image\n",
    "                g_encoded_outputs = layer(g_encoded_outputs)\n",
    "\n",
    "                # If this layer is used to calculate the loss save its outputs\n",
    "                if layer.name in LOSS_LAYERS:\n",
    "                    g_loss_outputs.append(g_encoded_outputs)\n",
    "                    \n",
    "            # 5. Calculate the content loss\n",
    "            c_per_replica_loss = self.content_loss(g_encoded_outputs, adain_outputs) # (N,W,H)\n",
    "            # Reduce the loss (we do this ourselves in order to be compatible with distributed training)\n",
    "            global_c_loss_size = tf.size(c_per_replica_loss) * self.distribute_strategy.num_replicas_in_sync\n",
    "            global_c_loss_size = tf.cast(global_c_loss_size, dtype=tf.float32)\n",
    "            c_loss = tf.nn.compute_average_loss(c_per_replica_loss, global_batch_size=global_c_loss_size)\n",
    "\n",
    "            assert len(g_loss_outputs) == len(s_loss_outputs)\n",
    "\n",
    "            # 6. Calculate style loss\n",
    "            s_loss = 0\n",
    "            for i in range(len(g_loss_outputs)):\n",
    "                s_per_replica_loss = self.style_loss(g_loss_outputs[i], s_loss_outputs[i]) # (N,)\n",
    "                # Reduce the loss (we do this ourselves in order to be compatible with distributed training)\n",
    "                global_s_loss_size = BATCH_SIZE * self.distribute_strategy.num_replicas_in_sync\n",
    "                s_loss += tf.nn.compute_average_loss(s_per_replica_loss, global_batch_size=global_s_loss_size)\n",
    "\n",
    "            # 7. Calculate the loss\n",
    "            loss = c_loss + s_loss*STYLE_WEIGHT\n",
    "\n",
    "        # 8. Apply gradient descent\n",
    "        if training:\n",
    "            gradients = tape.gradient(loss, self.decoder.trainable_variables)\n",
    "                    \n",
    "#             gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "#             tf.print('---')\n",
    "#             tf.print('glonorm', tf.linalg.global_norm(gradients))\n",
    "#             tf.print(list((i, tf.math.reduce_min(n), tf.math.reduce_max(n)) for i,n in enumerate(gradients)))\n",
    "#             tf.print(list((i, tf.math.reduce_min(n), tf.math.reduce_max(n)) for i,n in enumerate(self.decoder.trainable_variables)))\n",
    "#             tf.print('C_ENC --> ', tf.math.reduce_min(c_encoded_outputs), tf.math.reduce_max(c_encoded_outputs))\n",
    "#             tf.print('S_ENC --> ', tf.math.reduce_min(s_encoded_outputs), tf.math.reduce_max(s_encoded_outputs))\n",
    "#             tf.print('ADAIN --> ', tf.math.reduce_min(adain_outputs), tf.math.reduce_max(adain_outputs))\n",
    "#             tf.print('GEN_IMG --> ', tf.math.reduce_min(generated_imgs), tf.math.reduce_max(generated_imgs))\n",
    "#             tf.print('G_ENC --> ', tf.math.reduce_min(g_encoded_outputs), tf.math.reduce_max(g_encoded_outputs))\n",
    "#             tf.print('S_LOSS_E --> ', tf.math.reduce_min(s_loss_outputs[i]), tf.math.reduce_max(s_loss_outputs[i]))\n",
    "#             tf.print('G_LOSS_E --> ', tf.math.reduce_min(g_loss_outputs[i]), tf.math.reduce_max(g_loss_outputs[i]))\n",
    "\n",
    "            self.decoder.optimizer.apply_gradients(zip(gradients, self.decoder.trainable_variables))\n",
    "        \n",
    "        # 9. Update the metrics\n",
    "        self.c_loss_metric.update_state(c_loss)\n",
    "        self.s_loss_metric.update_state(s_loss)\n",
    "        self.loss_metric.update_state(loss)\n",
    "\n",
    "        return { m.name: m.result() for m in self.metrics }\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        c_encoded_outputs, s_encoded_outputs = data\n",
    "        \n",
    "        # The outputs of the encoded style images and the encoded generated images\n",
    "        # Retrieved from the selected encoder layers used for the loss\n",
    "        s_loss_outputs = []\n",
    "        g_loss_outputs = []\n",
    "            \n",
    "        # 1. Encode the content and style image\n",
    "        for layer in self.encoder.layers:\n",
    "            # Encode the content image\n",
    "            c_encoded_outputs = layer(c_encoded_outputs, training=False)\n",
    "            # Encode the style image\n",
    "            s_encoded_outputs = layer(s_encoded_outputs, training=False)\n",
    "\n",
    "            # If this layer is used to calculate the loss save its outputs\n",
    "            if layer.name in LOSS_LAYERS:\n",
    "                s_loss_outputs.append(s_encoded_outputs)\n",
    "\n",
    "        # 2. Adaptive Instance Normalization\n",
    "        adain_outputs = self.adain(c_encoded_outputs, s_encoded_outputs)\n",
    "\n",
    "        # 3. Decode the feature maps generated by AdaIN to get the final generated image\n",
    "        generated_imgs = self.decoder(adain_outputs, training=False)\n",
    "\n",
    "        # 4. Encode the generated image to calculate the loss\n",
    "        g_encoded_outputs = generated_imgs\n",
    "        for layer in self.encoder.layers:\n",
    "            # Encode the generated image\n",
    "            g_encoded_outputs = layer(g_encoded_outputs, training=False)\n",
    "\n",
    "            # If this layer is used to calculate the loss save its outputs\n",
    "            if layer.name in LOSS_LAYERS:\n",
    "                g_loss_outputs.append(g_encoded_outputs)\n",
    "\n",
    "        # 5. Calculate the content loss\n",
    "        c_per_replica_loss = self.content_loss(g_encoded_outputs, adain_outputs) # (N,W,H)\n",
    "        # Reduce the loss (we do this ourselves in order to be compatible with distributed training)\n",
    "        global_c_loss_size = tf.size(c_per_replica_loss) * self.distribute_strategy.num_replicas_in_sync\n",
    "        global_c_loss_size = tf.cast(global_c_loss_size, dtype=tf.float32)\n",
    "        c_loss = tf.nn.compute_average_loss(c_per_replica_loss, global_batch_size=global_c_loss_size)\n",
    "\n",
    "        assert len(g_loss_outputs) == len(s_loss_outputs)\n",
    "\n",
    "        # 6. Calculate style loss\n",
    "        s_loss = 0\n",
    "        for i in range(len(g_loss_outputs)):\n",
    "            s_per_replica_loss = self.style_loss(g_loss_outputs[i], s_loss_outputs[i]) # (N,)\n",
    "            # Reduce the loss (we do this ourselves in order to be compatible with distributed training)\n",
    "            global_s_loss_size = BATCH_SIZE * self.distribute_strategy.num_replicas_in_sync\n",
    "            s_loss += tf.nn.compute_average_loss(s_per_replica_loss, global_batch_size=global_s_loss_size)\n",
    "\n",
    "        # 7. Calculate the loss\n",
    "        loss = c_loss + s_loss*STYLE_WEIGHT\n",
    "\n",
    "        # 9. Update the metrics\n",
    "        self.c_loss_metric.update_state(c_loss)\n",
    "        self.s_loss_metric.update_state(s_loss)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        \n",
    "        return { m.name: m.result() for m in self.metrics }\n",
    "        \n",
    "    @tf.function\n",
    "    def predict_step(self, data):\n",
    "        content_imgs, style_imgs = data[0]\n",
    "        \n",
    "        # Ensure these are batched\n",
    "        assert len(content_imgs.shape) == 4\n",
    "        assert len(style_imgs.shape) == 4\n",
    "\n",
    "        content_imgs = vgg19_preprocess_input(content_imgs) / 255.0\n",
    "        style_imgs = vgg19_preprocess_input(style_imgs) / 255.0\n",
    "\n",
    "        c_encoded = content_imgs\n",
    "        s_encoded = style_imgs\n",
    "\n",
    "        # Encode the contents and styles\n",
    "        for layer in self.encoder.layers:\n",
    "            c_encoded = layer(c_encoded)\n",
    "            s_encoded = layer(s_encoded)\n",
    "            \n",
    "        # Apply adaptive batch normalization\n",
    "        adain_outputs = AdaIN()(c_encoded, s_encoded)\n",
    "        \n",
    "        # Decode the images to generate them\n",
    "        generated_imgs = self.decoder(adain_outputs)\n",
    "        generated_imgs = self.deprocess_vgg19(generated_imgs)\n",
    "        \n",
    "        return generated_imgs\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric, self.c_loss_metric, self.s_loss_metric]\n",
    "    \n",
    "    def deprocess_vgg19(self, imgs):\n",
    "        # Ensure they are batched\n",
    "        assert len(imgs.shape) == 4\n",
    "        \n",
    "        # Put back to 0...255\n",
    "        imgs *= 255.0\n",
    "        # Add mean\n",
    "        imgs += [103.939, 116.779, 123.68]\n",
    "        # BGR to RGB\n",
    "        imgs = imgs[..., ::-1]\n",
    "        # Clip\n",
    "        imgs = tf.clip_by_value(imgs, 0.0, 255.0)\n",
    "        # Cast\n",
    "        imgs = tf.cast(imgs, tf.uint8)\n",
    "\n",
    "        return imgs\n",
    "    \n",
    "    def save_architecture(self, log_dir):\n",
    "        # Ensure the log_dir exists\n",
    "        pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with GFile(os.path.join(log_dir, 'style_transfer_architecture.json'), 'w') as f:\n",
    "            f.write(self.to_json())\n",
    "            \n",
    "    def save_encoder(self, log_dir):\n",
    "        self.encoder.save(log_dir)\n",
    "    \n",
    "    def save_model(self, log_dir, **kwargs):\n",
    "        self.decoder.save(log_dir, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, log_dir=None):\n",
    "        model_found = bool(log_dir) and pathlib.Path(os.path.join(log_dir, 'style_transfer_architecture.json')).is_file()\n",
    "                \n",
    "        # If there isn't already a model create one from scratch and save it\n",
    "        if not model_found:\n",
    "            model = cls()\n",
    "            if log_dir:\n",
    "                model.save_architecture(log_dir)\n",
    "                model.save_encoder(os.path.join(log_dir, 'encoder'))\n",
    "            return model\n",
    "        \n",
    "        # Load the model's architecture\n",
    "        with tf.keras.utils.custom_object_scope({'StyleTransfer':cls, 'Conv2DReflectivePadding':Conv2DReflectivePadding}):\n",
    "            saved_json = GFile(os.path.join(log_dir, 'style_transfer_architecture.json'), 'r').read()\n",
    "            model = tf.keras.models.model_from_json(saved_json)\n",
    "            model.encoder = tf.keras.models.load_model(os.path.join(log_dir, 'encoder'))\n",
    "        \n",
    "        # Load the decoder's latest weights if there are any\n",
    "        ckpts = glob.glob(os.path.join(log_dir, 'weights', '*'))\n",
    "        if ckpts:\n",
    "            latest_ckpt = max(ckpts, key=os.path.getmtime)\n",
    "            print('Loading Checkpoint:', latest_ckpt)\n",
    "            model.decoder = tf.keras.models.load_model(latest_ckpt)\n",
    "            model.decoder_compiled = True\n",
    "            \n",
    "        return model\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'encoder': self.encoder,\n",
    "            'decoder': self.decoder\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config, **kwargs):\n",
    "        encoder = tf.keras.models.model_from_json(json.dumps(config.pop('encoder')))\n",
    "        decoder = tf.keras.models.model_from_json(json.dumps(config.pop('decoder')))\n",
    "\n",
    "        style_transfer = cls(encoder=encoder, decoder=decoder)\n",
    "        return style_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_epoch(log_dir=None):\n",
    "    if log_dir is None:\n",
    "        return 0\n",
    "\n",
    "    ckpts = glob.glob(os.path.join(log_dir, 'weights', '*'))\n",
    "\n",
    "    # If there are no latest checkpoints start from scratch\n",
    "    if not ckpts:\n",
    "        return 0\n",
    "    \n",
    "    latest_ckpt_path = max(ckpts, key=os.path.getmtime)\n",
    "    path = pathlib.PurePath(latest_ckpt_path)\n",
    "    # Checkpoint prefixes are stored as epoch_x so we split to get the epoch number\n",
    "    epoch = path.name.split('_')[-1]\n",
    "    \n",
    "    return int(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = None\n",
    "log_dir = 'testlogs/one_value_corrected_c_loss'\n",
    "\n",
    "st = StyleTransfer.load(log_dir)\n",
    "st.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=1e-07),\n",
    "    content_loss=MeanSquaredError(reduction='none'),\n",
    "    style_loss=StyleLoss(reduction='none')\n",
    ")\n",
    "\n",
    "# st.decoder.optimizer.lr = 0.0\n",
    "st.decoder.optimizer.lr = 4e-07\n",
    "\n",
    "# st.decoder = tf.keras.models.load_model(os.path.join(log_dir, 'tmp_lr_search', str(lrsearch.best_loss_index)))\n",
    "# st.decoder.optimizer.lr = lrsearch.history['lr'][lrsearch.best_loss_index - 1]\n",
    "\n",
    "ds, val_ds = get_dataset()\n",
    "\n",
    "callbacks = [\n",
    "#     EarlyStopping(monitor='loss', patience=3),\n",
    "#     ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=2),\n",
    "#     TensorBoard(log_dir=f'{log_dir}/tensorboard'),\n",
    "    SaveModel(log_dir=f'{log_dir}/weights')\n",
    "#     SaveWeightsSummary(f'{log_dir}/tensorboard/weights')`\n",
    "#     LRSearch(1e-10, 1e-01, 1, 100, log_dir, save=False)\n",
    "]\n",
    "\n",
    "last_epoch = get_last_epoch(log_dir)\n",
    "\n",
    "print('LR', st.decoder.optimizer.lr.numpy())\n",
    "\n",
    "epochs = 2000\n",
    "st.fit(ds, epochs=epochs+last_epoch, initial_epoch=last_epoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSearch(Callback):\n",
    "    def __init__(self, lr_min, lr_max, steps, epochs, log_dir, save=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.steps = steps\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.save = save\n",
    "\n",
    "        self.decay = (lr_max / lr_min) ** (1/(epochs*steps))\n",
    "        self.history = {'lr': [], 'loss': []}\n",
    "        self.best_loss = math.inf\n",
    "        self.best_lr = math.inf\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.model.decoder.optimizer.lr = self.lr = self.lr_min\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs):\n",
    "        self.lr *= self.decay\n",
    "        self.model.decoder.optimizer.lr = self.lr\n",
    "        \n",
    "        self.history['lr'].append(self.lr)\n",
    "        self.history['loss'].append(logs['loss'])\n",
    "        \n",
    "        if self.save:\n",
    "            self.model.save_weights(os.path.join(self.log_dir, 'tmp_lr_search', str(len(self.history['loss']) - 1)))\n",
    "        \n",
    "        if self.best_loss > logs['loss']:\n",
    "            self.best_loss = logs['loss']\n",
    "            self.best_loss_index = len(self.history['loss']) - 1\n",
    "            self.best_lr = self.history['lr'][self.best_loss_index - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min = 1e-06\n",
    "lr_max = 1e-02\n",
    "steps = 1\n",
    "epochs = 50\n",
    "\n",
    "decay = (lr_max / lr_min) ** (1/(epochs*steps))\n",
    "\n",
    "\n",
    "lr = lr_min\n",
    "\n",
    "x = list(i+1 for i in range(epochs))\n",
    "y = []\n",
    "\n",
    "for i in x:\n",
    "    lr *= decay\n",
    "    y.append(lr)\n",
    "\n",
    "plt.plot(x,y,'-')\n",
    "plt.ylabel('lr')\n",
    "plt.xlabel('steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(lrsearch.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsearch.history['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ran = list(i for i in lrsearch.history['loss'] if i > 140 and i < 150)\n",
    "# len(ran)\n",
    "\n",
    "min(lrsearch.history['loss'])\n",
    "lrsearch.history['loss'].index(97.63299560546875)\n",
    "lrsearch.history['lr'][47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevsearch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevsearch.append(callbacks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(lrsearch.history['loss'])\n",
    "lrsearch.history['loss'].index(3296092657549312.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsearch = prevsearch[-1]\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "start = 0\n",
    "end = 124\n",
    "plt.plot(lrsearch.history['lr'][start:end], lrsearch.history['loss'][start:end], '-')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('lr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_descent = math.inf\n",
    "best_lr = math.inf\n",
    "best_loss = math.inf\n",
    "best_loss_index = None\n",
    "\n",
    "# Find best loss\n",
    "for i in range(len(lrsearch.history['loss'])):\n",
    "    if best_loss > lrsearch.history['loss'][i]:\n",
    "        best_loss = lrsearch.history['loss'][i]\n",
    "        best_loss_index = i\n",
    "\n",
    "# for i in range(1, len(lrsearch.history['lr'])):\n",
    "#     slope = lrsearch.history['loss'][i] - lrsearch.history['loss'][i-1]\n",
    "#     if best_descent > slope:\n",
    "#         best_descent = slope\n",
    "#         best_lr = lrsearch.history['lr'][i - 1]\n",
    "#         loss = lrsearch.history['loss'][i]\n",
    "#         index = i\n",
    "        \n",
    "# print('best_descent', best_descent)\n",
    "# print('best_lr', best_lr)\n",
    "print('best_loss', best_loss)\n",
    "print('best_loss_index', best_loss_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best_lr', lrsearch.best_lr)\n",
    "print('best_loss', lrsearch.best_loss)\n",
    "print('best_loss_index', lrsearch.best_loss_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20\n",
    "end = 30\n",
    "plt.plot(lrsearch.history['lr'][start:end], lrsearch.history['loss'][start:end], 'o-')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('lr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsearch.history['loss'][27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,l in enumerate(st.decoder.layers):\n",
    "    print(i,l.name)\n",
    "    if 'conv' in l.name:\n",
    "        plt.hist(l.weights[0].numpy().flatten())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,l in enumerate(vgg19.layers):\n",
    "    print(i,l.name)\n",
    "    if 'conv' in l.name:\n",
    "        plt.hist(l.weights[0].numpy().flatten())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_fn = glob.glob('/home/jephthia/datasets/mscoco/unlabeled2017/train/*')[0]\n",
    "style_fn = glob.glob('/home/jephthia/datasets/wikiart/train/*')[0]\n",
    "print(content_fn)\n",
    "print(style_fn)\n",
    "\n",
    "def parseim(file_name):\n",
    "    img = tf.io.read_file(file_name)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224], antialias=True, method='nearest')\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "    return img\n",
    "\n",
    "c_imgs = parseim(content_fn)\n",
    "s_imgs = parseim(style_fn)\n",
    "\n",
    "# print(c_imgs)\n",
    "\n",
    "pred = st.predict((c_imgs, s_imgs), batch_size=1)\n",
    "im = pred[0]\n",
    "\n",
    "print('min', np.min(im))\n",
    "print('max', np.max(im))\n",
    "print('avg', np.mean(im))\n",
    "\n",
    "# print(im)\n",
    "# plt.hist(im.flatten())\n",
    "# plt.show()\n",
    "plt.hist(im.flatten())\n",
    "plt.show()\n",
    "c_imgs = tf.cast(c_imgs, tf.uint8)\n",
    "s_imgs = tf.cast(s_imgs, tf.uint8)\n",
    "\n",
    "plt.imshow(c_imgs[0])\n",
    "plt.show()\n",
    "plt.imshow(s_imgs[0])\n",
    "plt.show()\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycimg, mysimg = next(iter(ds))\n",
    "\n",
    "print('c image', np.min(mycimg.numpy()), np.max(mycimg.numpy()))\n",
    "print('s image', np.min(mysimg.numpy()), np.max(mysimg.numpy()))\n",
    "\n",
    "plt.hist(mycimg.numpy().flatten())\n",
    "plt.show()\n",
    "\n",
    "for l in st.encoder.layers:\n",
    "    mycimg = l(mycimg)\n",
    "    mysimg = l(mysimg)\n",
    "    \n",
    "print('encoded c image', np.min(mycimg.numpy()), np.max(mycimg.numpy()))\n",
    "print('encoded s image', np.min(mysimg.numpy()), np.max(mysimg.numpy()))\n",
    "\n",
    "myada = AdaIN()(mycimg, mysimg)\n",
    "\n",
    "print('adain', np.min(myada.numpy()), np.max(myada.numpy()))\n",
    "\n",
    "# declayers = iter(st.decoder.layers)\n",
    "\n",
    "gen = st.decoder(myada)\n",
    "\n",
    "# gen = next(declayers)(myada) # up\n",
    "# gen = next(declayers)(gen) # conv\n",
    "# gen = next(declayers)(gen) # up\n",
    "# gen = next(declayers)(gen) # conv\n",
    "# gen = next(declayers)(gen) # up\n",
    "# gen = next(declayers)(gen) # conv\n",
    "# gen = next(declayers)(gen) # output\n",
    "\n",
    "# for u in st.decoder.layers:\n",
    "#     if 'conv' in u.name:\n",
    "#         plt.hist(u.weights[0].numpy().flatten())\n",
    "#         plt.show()\n",
    "\n",
    "plt.hist(gen.numpy().flatten())\n",
    "plt.show()\n",
    "print('generated', np.min(gen.numpy()), np.max(gen.numpy()))\n",
    "\n",
    "\n",
    "finalimg = st.deprocess_vgg19(gen)\n",
    "# finalimg = gen * 255.0\n",
    "# finalimg += [103.939, 116.779, 123.68]\n",
    "# finalimg = finalimg[..., ::-1]\n",
    "# finalimg = tf.clip_by_value(finalimg, 0.0, 255.0)\n",
    "# finalimg = tf.cast(finalimg, tf.uint8)\n",
    "\n",
    "plt.hist(finalimg.numpy().flatten())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(finalimg[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
