{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Embedding, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow.keras.applications import EfficientNetB1\n",
    "from tensorflow.io.gfile import GFile\n",
    "from tensorflow.strings import unicode_split\n",
    "from tensorflow.data import Dataset, TextLineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_net = EfficientNetB1(input_shape=(100,100,3), include_top=False, weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 84\n",
    "embedding_size = 256\n",
    "sequence_length = 100\n",
    "\n",
    "inputs = Input((sequence_length))\n",
    "\n",
    "outs = Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=sequence_length)(inputs)\n",
    "\n",
    "# outs = GRU()(inputs)\n",
    "\n",
    "svg_net = Model(inputs=inputs,outputs=outs)\n",
    "\n",
    "svg_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 132) (2, 100, 100, 3)\n",
      "(2, 132) (2, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        self.START_TOKEN = '[SOS]'\n",
    "        self.END_TOKEN = '[EOS]'\n",
    "        self.vocab = list(sorted(set(string.printable))) + [self.START_TOKEN, self.END_TOKEN]\n",
    "        self.chars_to_ids = StringLookup(vocabulary=self.vocab)\n",
    "\n",
    "    def load_dataset(self):\n",
    "        ds = TextLineDataset(str(pathlib.Path(self.log_dir, 'file_names.txt')))\n",
    "        ds = ds.take(5)\n",
    "        ds = ds.map(self.parse_svg_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        ds = ds.padded_batch(2, drop_remainder=True)\n",
    "        \n",
    "        return ds\n",
    "            \n",
    "    def parse_svg_img(self, file_name):\n",
    "        svg_path = tf.strings.join([self.log_dir, '/svgs/', file_name, '.svg'])\n",
    "        img_path = tf.strings.join([self.log_dir, '/imgs/', file_name, '.png'])\n",
    "\n",
    "        svg = tf.io.read_file(svg_path)\n",
    "        svg = tf.concat([[self.START_TOKEN], unicode_split(svg, 'UTF-8'), [self.END_TOKEN]], axis=0)\n",
    "        svg = self.chars_to_ids(svg)\n",
    "        \n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.io.decode_png(img, channels=3)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = img / 255.0\n",
    "        \n",
    "        return svg, img\n",
    "\n",
    "dm = DataManager('dataset')\n",
    "\n",
    "ds = dm.load_dataset()\n",
    "\n",
    "for _ in ds:\n",
    "    print(_[0].shape, _[1].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
