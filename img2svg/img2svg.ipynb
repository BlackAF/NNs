{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GRU, Embedding, Input, Dense, GlobalAveragePooling2D, Add\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow.keras.applications import EfficientNetB1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.io.gfile import GFile\n",
    "from tensorflow.strings import unicode_split\n",
    "from tensorflow.data import Dataset, TextLineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 7s 261ms/step - loss: 3748.1405\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 1s 262ms/step - loss: 3715.1595\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 1s 265ms/step - loss: 3653.5775\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 1s 271ms/step - loss: 3653.5277\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 231ms/step - loss: 3653.5260\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 1s 347ms/step - loss: 3653.5243\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 3653.5239\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 1s 254ms/step - loss: 3653.5239\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 1s 255ms/step - loss: 3653.5239\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 247ms/step - loss: 3653.5239\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 233ms/step - loss: 3653.5239\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 244ms/step - loss: 3653.5239\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 3653.5239\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 255ms/step - loss: 3653.5239\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 250ms/step - loss: 3653.5239\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 249ms/step - loss: 3653.5239\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 1s 296ms/step - loss: 3653.5239\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 1s 266ms/step - loss: 3653.5239\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 232ms/step - loss: 3653.5239\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 244ms/step - loss: 3653.5239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5748e6ea60>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IMG2SVG(Model):\n",
    "    def __init__(self, vocab_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.img_encoder = self.load_img_encoder()\n",
    "        self.svg_encoder = self.load_svg_encoder()\n",
    "        self.add = Add()\n",
    "        self.dense = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def load_img_encoder(self):\n",
    "        img_encoder = Sequential()\n",
    "\n",
    "        img_encoder.add(EfficientNetB1(input_shape=(100,100,3), include_top=False, weights=None))\n",
    "        img_encoder.add(GlobalAveragePooling2D())\n",
    "\n",
    "        return img_encoder\n",
    "    \n",
    "    def load_svg_encoder(self):\n",
    "        svg_encoder = Sequential()\n",
    "\n",
    "        svg_encoder.add(Embedding(input_dim=self.vocab_size, output_dim=C.EMBEDDING_SIZE))\n",
    "        svg_encoder.add(GRU(1280, return_sequences=True))\n",
    "\n",
    "        return svg_encoder\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        svg, img = x\n",
    "\n",
    "        img_input = self.img_encoder(img, training=training)\n",
    "        svg_input = self.svg_encoder(svg, training=training)\n",
    "\n",
    "        x = self.add([img_input, svg_input])\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self(x, training=True) # (BATCH,SEQUENCE_LEN,VOCAB_SIZE) \n",
    "            # Choose the highest probability for each char\n",
    "            preds = tf.math.reduce_max(preds, axis=-1) # (BATCH,SEQUENCE_LEN)\n",
    "            loss = self.compiled_loss(y, preds)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "\n",
    "        return { m.name: m.result() for m in self.metrics }\n",
    "        \n",
    "img2svg = IMG2SVG(vocab_size=dm.vocab_size)\n",
    "\n",
    "img2svg.compile(Adam(lr=1e-03), loss='mse')\n",
    "\n",
    "img2svg.fit(ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 132) (2, 100, 100, 3) (2, 132)\n",
      "(2, 132) (2, 100, 100, 3) (2, 132)\n"
     ]
    }
   ],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        self.START_TOKEN = '[SOS]'\n",
    "        self.END_TOKEN = '[EOS]'\n",
    "        self.vocab = list(sorted(set(string.printable))) + [self.START_TOKEN, self.END_TOKEN]\n",
    "        self.chars_to_ids = StringLookup(vocabulary=self.vocab)\n",
    "        self.vocab_size = self.chars_to_ids.vocab_size()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        ds = TextLineDataset(str(pathlib.Path(self.log_dir, 'file_names.txt')))\n",
    "        ds = ds.take(5)\n",
    "        ds = ds.map(self.parse_svg_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        ds = ds.padded_batch(2, drop_remainder=True)\n",
    "        \n",
    "        return ds\n",
    "            \n",
    "    def parse_svg_img(self, file_name):\n",
    "        svg_path = tf.strings.join([self.log_dir, '/svgs/', file_name, '.svg'])\n",
    "        img_path = tf.strings.join([self.log_dir, '/imgs/', file_name, '.png'])\n",
    "\n",
    "        svg = tf.io.read_file(svg_path)\n",
    "        svg = tf.concat([[self.START_TOKEN], unicode_split(svg, 'UTF-8'), [self.END_TOKEN]], axis=0)\n",
    "        svg = self.chars_to_ids(svg)\n",
    "        \n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.io.decode_png(img, channels=3)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = img / 255.0\n",
    "        \n",
    "        return (svg, img), svg\n",
    "\n",
    "dm = DataManager('dataset')\n",
    "\n",
    "ds = dm.load_dataset()\n",
    "\n",
    "for _,__ in ds:\n",
    "    print(_[0].shape, _[1].shape, __.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C:\n",
    "    EMBEDDING_SIZE = 256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
