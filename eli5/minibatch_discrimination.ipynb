{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.shape[1]\n",
    "kernel_dims = 1\n",
    "\n",
    "features = tf.reshape(X, (-1, num_features, kernel_dims)) # NxBxC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a dimension so that when we subtract this\n",
    "with the rest of the batch we can take advantage\n",
    "of broadcasting to calculate all substractions\n",
    "at once in a big tensor. So it'll go from 1xNxBxC\n",
    "to a NxNxBxC which is basically saying that we want\n",
    "our NxBxC input tensor copied N times. So we'll\n",
    "end up with N identical NxBxC tensors. So for example\n",
    "let's assume that\n",
    "```\n",
    "[\n",
    "  [ [a], [b], [c] ],\n",
    "  [ [d], [e], [f] ]\n",
    "]\n",
    "```\n",
    "is our input (NxBxC=2x3x1) once we do Mi - Mj, then\n",
    "Mi will be broadcasted to be a (NxNxBxC=2x2x3x1)\n",
    "```\n",
    "[\n",
    "  [\n",
    "    [ [a], [b], [c] ],\n",
    "    [ [d], [e], [f] ]\n",
    "  ],\n",
    "  [\n",
    "    [ [a], [b], [c] ],\n",
    "    [ [d], [e], [f] ]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "which is the same tensor Mi copied N times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mi = tf.expand_dims(features, axis=0) # 1xNxBxC\n",
    "# Use this to visualize what it will look like after broadcasting\n",
    "# Mi = tf.broadcast_to(Mi, (X.shape[1], num_samples, num_features, kernel_dims)) # NxNxBxC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same idea as above but this time we\n",
    "want the final tensor to be arranged slightly differently.\n",
    "By adding a dimension in axis 1 we are basically saying\n",
    "that we want N tensors but that in each tensor we want\n",
    "one matrix that corresponds to 1 sample in the batch.\n",
    "So if you had for example the same as above:\n",
    "```\n",
    "[\n",
    "  [ [a], [b], [c] ],\n",
    "  [ [d], [e], [f] ]\n",
    "]\n",
    "```\n",
    "after this exansion you would now have\n",
    "```\n",
    "[\n",
    "  [\n",
    "    [ [a], [b], [c] ]\n",
    "  ],\n",
    "  [\n",
    "    [ [d], [e], [f] ]\n",
    "  ]\n",
    "] # (Nx1xBxC=2x1x3x1)\n",
    "```\n",
    "you can think of it as having separated each input in\n",
    "the batch in its own tensor. So once we do Mi - Mj\n",
    "it will be broadcasted to a (NxNxBxC=2x2x3x1) giving you\n",
    "```\n",
    "[\n",
    "  [\n",
    "    [ [a], [b], [c] ],\n",
    "    [ [a], [b], [c] ]\n",
    "  ],\n",
    "  [\n",
    "    [ [d], [e], [f] ],\n",
    "    [ [d], [e], [f] ]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "which is basically copying each input N times for all\n",
    "the samples in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mj = tf.expand_dims(features, axis=1) # Nx1xBxC\n",
    "# Use this to visualize what it will look like after broadcasting\n",
    "# Mj = tf.broadcast_to(Mj, (num_samples, Mj.shape[0], num_features, kernel_dims)) # NxNxBxC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff = tnp.abs(Mi - Mj) # NxNxBxC\n",
    "\n",
    "# Sum each tube\n",
    "norm = tnp.sum(abs_diff, axis=3) # NxNxB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tube contains one image that has been subtracted with\n",
    "another image in the batch, so all the subtractions of one\n",
    "image with the others in the batch are stored in the 1st\n",
    "dimension (`shape[0]`) i.e All image_1 calcs are stored in\n",
    "`[0,:,:]`, image_2 in `[1,:,:]` etc So we sum accross the 1st\n",
    "dimensation to get the sum of all image subtraction for all\n",
    "image_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tnp.sum(tnp.exp(-norm), axis=0) # NxB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack those differences with your input. So for example if\n",
    "your input is\n",
    "```\n",
    "[\n",
    "  [ [z], [y] ],\n",
    "  [ [x], [w] ]\n",
    "] # (NxB=2x2)\n",
    "```\n",
    "you would now have something like this\n",
    "```\n",
    "[\n",
    "  [ [z], [y], [a], [b], [c] ],\n",
    "  [ [x], [w], [d], [e], [f] ]\n",
    "] # (NxB1+B2=2x5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_to_next_layer = Concatenate(axis=1)((X, outputs)) # Nx(B+X.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from tensorflow.keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With comments (Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_discrimination(X):\n",
    "    num_features = X.shape[1]\n",
    "    kernel_dims = 1\n",
    "\n",
    "    features = tf.reshape(X, (-1, num_features, kernel_dims)) # NxBxC\n",
    "    \n",
    "    # We add a dimension so that when we subtract this\n",
    "    # with the rest of the batch we can take advantage\n",
    "    # of broadcasting to calculate all substractions\n",
    "    # at once in a big tensor. So it'll go from 1xNxBxC\n",
    "    # to a NxNxBxC which is basically saying that we want\n",
    "    # our NxBxC input tensor copied N times. So we'll\n",
    "    # end up with N identical NxBxC tensors. So for example\n",
    "    # let's assume that\n",
    "    # [\n",
    "    #   [ [a], [b], [c] ],\n",
    "    #   [ [d], [e], [f] ]\n",
    "    # ]\n",
    "    # is our input (NxBxC=2x3x1) once we do Mi - Mj, then\n",
    "    # Mi will be broadcasted to be a (NxNxBxC=2x2x3x1)\n",
    "    # [\n",
    "    #  [\n",
    "    #   [ [a], [b], [c] ],\n",
    "    #   [ [d], [e], [f] ]\n",
    "    #  ],\n",
    "    #  [\n",
    "    #   [ [a], [b], [c] ],\n",
    "    #   [ [d], [e], [f] ]\n",
    "    #  ]\n",
    "    # ]\n",
    "    # which is the same tensor Mi copied N times.\n",
    "    Mi = tf.expand_dims(features, axis=0) # 1xNxBxC\n",
    "    # Use this to visualize what it will look like after broadcasting\n",
    "    # Mi = tf.broadcast_to(Mi, (X.shape[1], num_samples, num_features, kernel_dims)) # NxNxBxC\n",
    "\n",
    "    # This is the same idea as above but this time we\n",
    "    # want the final tensor to be arranged slightly differently.\n",
    "    # By adding a dimension in axis 1 we are basically saying\n",
    "    # that we want N tensors but that in each tensor we want\n",
    "    # one matrix that corresponds to 1 sample in the batch.\n",
    "    # So if you had for example the same as above:\n",
    "    # [\n",
    "    #   [ [a], [b], [c] ],\n",
    "    #   [ [d], [e], [f] ]\n",
    "    # ] after this exansion you would now have\n",
    "    # [\n",
    "    #  [\n",
    "    #   [ [a], [b], [c] ]\n",
    "    #  ],\n",
    "    #  [\n",
    "    #   [ [d], [e], [f] ]\n",
    "    #  ]\n",
    "    # ] (Nx1xBxC=2x1x3x1)\n",
    "    # you can think of it as having separated each input in\n",
    "    # the batch in its own tensor. So once we do Mi - Mj\n",
    "    # it will be broadcasted to a (NxNxBxC=2x2x3x1) giving you\n",
    "    # [\n",
    "    #  [\n",
    "    #   [ [a], [b], [c] ],\n",
    "    #   [ [a], [b], [c] ]\n",
    "    #  ],\n",
    "    #  [\n",
    "    #   [ [d], [e], [f] ],\n",
    "    #   [ [d], [e], [f] ]\n",
    "    #  ]\n",
    "    # ] which is basically copying each input N times for all\n",
    "    # the samples in the batch\n",
    "    Mj = tf.expand_dims(features, axis=1) # Nx1xBxC\n",
    "    # Use this to visualize what it will look like after broadcasting\n",
    "    # Mj = tf.broadcast_to(Mj, (num_samples, Mj.shape[0], num_features, kernel_dims)) # NxNxBxC\n",
    "\n",
    "    abs_diff = tnp.abs(Mi - Mj) # NxNxBxC\n",
    "\n",
    "    # Sum each tube\n",
    "    norm = tnp.sum(abs_diff, axis=3) # NxNxB\n",
    "\n",
    "    # Each tube contains one image that has been subtracted with\n",
    "    # another image in the batch, so all the subtractions of one\n",
    "    # image with the others in the batch are stored in the 1st\n",
    "    # dimension (shape[0]) i.e All image_1 calcs are stored in\n",
    "    # [0,:,:], image_2 in [1,:,:] etc So we sum accross the 1st\n",
    "    # dimensation to get the sum of all image subtraction for all\n",
    "    # image_i\n",
    "    outputs = tnp.sum(tnp.exp(-norm), axis=0) # NxB\n",
    "\n",
    "    # Stack those differences with your input. So for example if\n",
    "    # your input is\n",
    "    # [\n",
    "    #   [ [z], [y] ],\n",
    "    #   [ [x], [w] ]\n",
    "    # ] (NxB=2x2)\n",
    "    # you would now have something like this\n",
    "    # [\n",
    "    #   [ [z], [y], [a], [b], [c] ],\n",
    "    #   [ [x], [w], [d], [e], [f] ]\n",
    "    # ] (NxB1+B2=2x5)\n",
    "    return Concatenate(axis=1)((X, outputs)) # Nx(B+X.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without comments (Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_discrimination(X):\n",
    "    num_features = X.shape[1]\n",
    "    kernel_dims = 1\n",
    "\n",
    "    features = tf.reshape(X, (-1, num_features, kernel_dims)) # NxBxC\n",
    "\n",
    "    Mi = tf.expand_dims(features, axis=0) # 1xNxBxC\n",
    "  \n",
    "    Mj = tf.expand_dims(features, axis=1) # Nx1xBxC\n",
    "\n",
    "    abs_diff = tnp.abs(Mi - Mj) # NxNxBxC\n",
    "\n",
    "    norm = tnp.sum(abs_diff, axis=3) # NxNxB\n",
    "\n",
    "    outputs = tnp.sum(tnp.exp(-norm), axis=0) # NxB\n",
    "\n",
    "    return Concatenate(axis=1)((X, outputs)) # Nx(B+X.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented as Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff ndarray<tf.Tensor(\n",
      "[[[[0]]\n",
      "\n",
      "  [[1]]]\n",
      "\n",
      "\n",
      " [[[1]]\n",
      "\n",
      "  [[0]]]], shape=(2, 2, 1, 1), dtype=int32)>\n",
      "norm ndarray<tf.Tensor(\n",
      "[[[0]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [0]]], shape=(2, 2, 1), dtype=int64)>\n",
      "ndarray<tf.Tensor(\n",
      "[[[1.        ]\n",
      "  [0.36787944]]\n",
      "\n",
      " [[0.36787944]\n",
      "  [1.        ]]], shape=(2, 2, 1), dtype=float64)>\n",
      "ndarray<tf.Tensor(\n",
      "[[1.         0.36787944]\n",
      " [2.         0.36787944]], shape=(2, 2), dtype=float64)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class MinibatchDiscrimination(Layer):\n",
    "    def __init__(self, kernel_dims, **kwargs):\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "        self.kernel_dims = kernel_dims\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_features = input_shape[-1]\n",
    "    \n",
    "    def call(self, X):\n",
    "        features = tf.reshape(X, (-1, self.in_features, self.kernel_dims)) # NxBxC\n",
    "\n",
    "        Mi = tf.expand_dims(features, axis=0) # 1xNxBxC\n",
    "\n",
    "        Mj = tf.expand_dims(features, axis=1) # Nx1xBxC\n",
    "\n",
    "        abs_diff = tnp.abs(Mi - Mj) # NxNxBxC\n",
    "        print('diff', abs_diff)\n",
    "\n",
    "        norm = tnp.sum(abs_diff, axis=3) # NxNxB\n",
    "        print('norm', norm)\n",
    "    \n",
    "        print(tnp.exp(-norm))\n",
    "        outputs = tnp.sum(tnp.exp(-norm), axis=0) # NxB\n",
    "\n",
    "        return tnp.concatenate((X, outputs), axis=1) # Nx(B+X.shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(MinibatchDiscrimination, self).get_config()\n",
    "        config.update({ 'kernel_dims': self.kernel_dims })\n",
    "        return config\n",
    "    \n",
    "layer = MinibatchDiscrimination(1)\n",
    "\n",
    "outputs = layer(tf.constant([[1],[2]]))\n",
    "\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
